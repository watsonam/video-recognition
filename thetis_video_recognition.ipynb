{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b11fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ca00d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91b96873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import config\n",
    "\n",
    "# Hide GPU from visible devices\n",
    "config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d138bb",
   "metadata": {},
   "source": [
    "## Video Preparation\n",
    "\n",
    "Down sample number of images by a factor of 4. Save down 16 images from each, taking the images only during the start and end of the shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "handed-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames_from_video(class_,video_path):\n",
    "# frame shape : (480, 640, 3)\n",
    "    print(f'Capturing frames from {video_path}')\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    number_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f'Video has number of frames: {number_frames}')\n",
    "    if number_frames == 0:\n",
    "        # Skip this file.\n",
    "        return\n",
    "    # we only want to save around a quarter of the video frames.\n",
    "    output_frame = round(number_frames / 20) \n",
    "    current_frame = 0\n",
    "    while(cap.isOpened() and current_frame != (number_frames)): \n",
    "        current_frame += 1\n",
    "        # capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if ret: # check ! (some webcam's need a \"warmup\")\n",
    "            file_name = os.path.split(video_path)[1]\n",
    "            file_suffix = file_name.split('.')[0]\n",
    "            name = os.path.join('/Users/adamwatson/Desktop/TENNIS/InputFrames/',class_,f'{file_suffix}_{current_frame}.jpg')\n",
    "            # save every forth frame\n",
    "            if current_frame % output_frame == 0:\n",
    "#                 print(f'Saving frame to: {name}')\n",
    "                cv2.imwrite(name, frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    # When everything is done release the capture\n",
    "    output_frame = frame\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd1000",
   "metadata": {},
   "source": [
    "Testing loading in a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "statistical-representation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('/Users/adamwatson/Desktop/TENNIS/InputData/forehand/p12_fvolley_s1.avi')\n",
    "number_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(number_frames)\n",
    "ret, frame = cap.read()\n",
    "#Display the resulting frame\n",
    "cv2.imshow(my_video_name+' frame '+ str(frame_seq),gray)\n",
    "\n",
    "#Set waitKey \n",
    "cv2.waitKey()\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6538390",
   "metadata": {},
   "source": [
    "# Video to Frames\n",
    "\n",
    "Convert tennis video dataset to frame images, reducing the number of frames by a factor of 4 to make processing more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df8001bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create the image folders ready for processing.\n",
    "video_root = '/Users/adamwatson/Desktop/TENNIS/InputData/'\n",
    "classes = ['backhand','backhand2hands','forehand','service','smash']\n",
    "images_root = '/Users/adamwatson/Desktop/TENNIS/InputFrames/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "wrapped-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_ in classes:\n",
    "    class_path = os.path.join(images_root,class_)\n",
    "    print(class_path)\n",
    "    if not os.path.exists(class_path):\n",
    "        try:\n",
    "            os.makedirs(class_path)\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e95667d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "no_videos = 0\n",
    "for class_ in classes:\n",
    "    for root, dirs, files in os.walk(os.path.join(video_root,class_), topdown=False):\n",
    "        for name in files:\n",
    "            no_videos += 1\n",
    "            get_frames_from_video(class_,os.path.join(root, name))\n",
    "            print(f'Videos processed: {no_videos}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "903e9e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename files into alphanumeric order\n",
    "# for class_ in classes:\n",
    "#     no_videos = 0\n",
    "#     print(class_)\n",
    "#     class_dir = os.path.join(images_root,class_)\n",
    "#     print(class_dir)\n",
    "#     for root, dirs, files in os.walk(class_dir):\n",
    "#         # Sort list of files based on last modification time in ascending order\n",
    "#         list_of_files = sorted(files)\n",
    "#         for name in list_of_files:\n",
    "#             no_videos += 1\n",
    "#             rename_dir = os.path.join(class_dir,f'{class_}_image_{no_videos}.jpg')\n",
    "#             print(rename_dir)\n",
    "#             os.rename(os.path.join(class_dir,name), os.path.join(class_dir,f'{class_}_image_{no_videos}.jpg'))\n",
    "#     print(f'Frames processed for class {class_}: {no_videos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48bda241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_frames_from_video('forehand','/Users/adamwatson/Desktop/TENNIS/InputData/forehand/p12_fvolley_s1.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e8ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53ea28d8",
   "metadata": {},
   "source": [
    "## Load in training data.\n",
    "\n",
    "Use the keras preprocessing function to load in test files from directory. Depending on the folder name, this will assign the training data a class based on it's root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d23b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# img_width = 480\n",
    "# img_height = 640\n",
    "# batch_size = 32\n",
    "# train_dir = '/Users/adamwatson/Desktop/TENNIS/InputFrames'\n",
    "# train_set = image_dataset_from_directory(\n",
    "#     train_dir,\n",
    "#     label_mode='categorical',\n",
    "#     batch_size=batch_size,\n",
    "#     image_size=(img_width, img_height),\n",
    "#     shuffle=True,\n",
    "#     seed=123)\n",
    "# #     validation_split=0.2,\n",
    "# #     subset=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "196e9402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_set.class_names)\n",
    "# print(type(train_set))\n",
    "# print(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb248b4",
   "metadata": {},
   "source": [
    "## Load in training data using ImageDataGenerator\n",
    "\n",
    "Use the keras preprocessing function to load in test files from directory. Depending on the folder name, this will assign the training data a class based on it's root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4a916a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22983 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "ROWS=480\n",
    "COLS=600\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow import test\n",
    "\n",
    "train_idg = ImageDataGenerator(\n",
    "                               preprocessing_function=preprocess_input)\n",
    "train_gen = train_idg.flow_from_directory(\n",
    "    '/Users/adamwatson/Desktop/TENNIS/InputFrames/',\n",
    "    target_size=(ROWS, COLS),\n",
    "    batch_size = 16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14773b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.image.DirectoryIterator at 0x16c51ed90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68763a1c",
   "metadata": {},
   "source": [
    "## Create new model\n",
    "\n",
    "Pre-trained CNN network such as InceptionV3, extracts features from each video frame and passes them to a softmax layer that outputs probabilities corresponding to each class. The input video is classified by averaging all the probabilities of each frame in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d51368b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# restrict GPU usage to restrict OOM issues.\n",
    "from tensorflow.config.experimental import list_physical_devices\n",
    "gpus = list_physical_devices('gpu')\n",
    "print(len(gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d1aec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
    "\n",
    "# USE FLATTEN LAYER.\n",
    "\n",
    "input_shape = (ROWS, COLS, 3)\n",
    "nclass = len(train_gen.class_indices)\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', \n",
    "                                include_top=False, \n",
    "                                input_shape=(ROWS, COLS,3))\n",
    "# Freeze inception layers (don't train these layers).\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0dfe17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model = Sequential()\n",
    "add_model.add(base_model)\n",
    "# add a dropout layer here\n",
    "# add_model.add(Flatten())\n",
    "# find out what global avg pooling 2D does.\n",
    "# add a validation split.\n",
    "add_model.add(Dropout(rate=0.3))\n",
    "add_model.add(GlobalAveragePooling2D())\n",
    "add_model.add(Dense(nclass, \n",
    "                    activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a48e293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Functional)    (None, 13, 17, 2048)      21802784  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 13, 17, 2048)      0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 21,806,882\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = add_model\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizers.SGD(learning_rate=0.01, \n",
    "                                       momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efb8c45",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "727902ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-28 19:40:44.367119: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-11-28 19:40:44.368521: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 137/1437 [=>............................] - ETA: 43:53 - loss: 0.9360 - accuracy: 0.5693"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r2/wfnjkdx500sgmjp666kctn980000gn/T/ipykernel_59913/1422137599.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_gen, epochs=5, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "450c2183",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r2/wfnjkdx500sgmjp666kctn980000gn/T/ipykernel_59913/40736445.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df899ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
